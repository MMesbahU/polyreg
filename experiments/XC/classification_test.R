
abalone <- read.csv("~/Desktop/project/abalone.data.txt",header=FALSE)
abalone$V1M <- ifelse(abalone$V1 == "M", 1, 0)
abalone$V1F <- ifelse(abalone$V1 == "F", 1, 0)
abalone <- abalone[,-1]
abalone <- abalone[,c(9,10,1:8)]
colnames(abalone)[10] <- "y"
ab <- abalone[abalone$y == 8 | abalone$y == 9 | abalone$y == 10 | abalone$y == 11, ]
str(ab)
# 'data.frame':	2378 obs. of  10 variables:
#   $ V1M: num  0 1 0 1 1 1 0 0 0 1 ...
# $ V1F: num  1 0 0 0 0 0 1 1 1 0 ...
# $ V2 : num  0.53 0.44 0.425 0.475 0.43 0.49 0.535 0.47 0.44 0.45 ...
# $ V3 : num  0.42 0.365 0.3 0.37 0.35 0.38 0.405 0.355 0.34 0.32 ...
# $ V4 : num  0.135 0.125 0.095 0.125 0.11 0.135 0.145 0.1 0.1 0.1 ...
# $ V5 : num  0.677 0.516 0.351 0.509 0.406 ...
# $ V6 : num  0.257 0.215 0.141 0.216 0.168 ...
# $ V7 : num  0.1415 0.114 0.0775 0.1125 0.081 ...
# $ V8 : num  0.21 0.155 0.12 0.165 0.135 0.19 0.205 0.185 0.13 0.115 ...
# $ y  : int  9 10 8 9 10 11 10 10 10 9 ...



t1 <- xvalPoly(ab,3,2,"glm")
# [1] 0.3907563 0.4180672 0.3697479
t2 <- xvalPoly(ab,3,2,"glm",0.8,TRUE, 0.9)
# [1] 0.3760504 0.4012605 0.3697479
t3 <- xvalPoly(ab,3,2,"glm",0.8,TRUE, 0.9, "one")
# [1] 0.3676471 0.4012605 0.3697479
t4 <- xvalPoly(ab,4,3,"glm")
# [1] 0.3907563 0.4180672 0.3739496 0.2920168
t5 <- xvalPoly(ab,4,3,"glm",0.8,TRUE, 0.9)
# [1] 0.3760504 0.4012605 0.3697479 0.3655462
t6 <- xvalPoly(ab,4,3,"glm",0.8,TRUE, 0.9, "one")
# [1] 0.3676471 0.4012605 0.3697479 0.3697479
t7 <- xvalPoly(ab,4,3,"glm",glmMethod = "one")
# [1] 0.3886555 0.4180672 0.3403361 0.2794118


library(nnet)
set.seed(500)
n <- nrow(ab)
ntrain <- round(0.8*n)
trainidxs <- sample(1:n, ntrain, replace = FALSE)
nynn1 <- nnet(as.factor(y)~., data=ab[trainidxs,],size=10,maxit=10000,decay=.001)
mean(ab[-trainidxs,]$y == predict(nynn1, ab[-trainidxs, ], type = "class"))
# [1] 0.3823529
nynn2 <- nnet(as.factor(y)~., data=ab[trainidxs,],size=20,maxit=10000,decay=.001)
mean(ab[-trainidxs,]$y == predict(nynn2, ab[-trainidxs, ], type = "class"))
# [1] 0.3634454
nynn3 <- nnet(as.factor(y)~., data=ab[trainidxs,],size=10,maxit=10000,decay=.005)
mean(ab[-trainidxs,]$y == predict(nynn3, ab[-trainidxs, ], type = "class"))
# [1] 0.3718487
nynn4 <- nnet(as.factor(y)~., data=ab[trainidxs,],size=5,maxit=10000,decay=.001)
mean(ab[-trainidxs,]$y == predict(nynn4, ab[-trainidxs, ], type = "class"))
# [1] 0.4159664
nynn5 <- nnet(as.factor(y)~., data=ab[trainidxs,],size=3,maxit=10000,decay=.001)
mean(ab[-trainidxs,]$y == predict(nynn5, ab[-trainidxs, ], type = "class"))
# [1] 0.3970588



dtrain <- read.csv("~/Desktop/project/optdigits.tra.txt",header=FALSE)
dtest <- read.csv("~/Desktop/project/optdigits.tes.txt",header = FALSE)
str(dtrain)
# 'data.frame':	3823 obs. of  65 variables:
#  $ V1 : int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V2 : int  1 0 0 0 0 0 0 0 0 0 ...
#  $ V3 : int  6 10 8 0 5 11 1 8 15 3 ...
#  $ V4 : int  15 16 15 3 14 16 11 10 2 13 ...
#  $ V5 : int  12 6 16 11 4 10 13 8 14 13 ...
#  $ V6 : int  1 0 13 16 0 1 11 7 13 2 ...
#  $ V7 : int  0 0 0 0 0 0 7 2 2 0 ...
#  $ V8 : int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V9 : int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V10: int  7 7 1 0 0 4 0 1 0 6 ...
#  $ V11: int  16 16 11 5 13 16 9 15 16 16 ...
#  $ V12: int  6 8 9 16 8 10 14 14 15 12 ...
#  $ V13: int  6 16 11 11 0 15 6 12 12 10 ...
#  $ V14: int  10 5 16 13 0 8 4 12 13 8 ...
#  $ V15: int  0 0 1 7 0 0 3 4 8 0 ...
#  $ V16: int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V17: int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V18: int  8 11 0 3 3 4 0 7 2 9 ...
#  $ V19: int  16 16 0 15 14 16 16 15 16 15 ...
#  $ V20: int  2 0 0 8 4 3 12 12 12 12 ...
#  $ V21: int  0 6 7 1 0 11 16 5 1 16 ...
#  $ V22: int  11 14 14 15 0 13 15 0 6 6 ...
#  $ V23: int  2 3 0 6 0 0 2 0 10 0 ...
#  $ V24: int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V25: int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V26: int  5 12 0 11 6 1 5 5 7 10 ...
#  $ V27: int  16 12 3 16 16 14 16 14 15 16 ...
#  $ V28: int  3 0 4 16 14 6 10 12 3 16 ...
#  $ V29: int  0 0 14 16 9 9 4 15 0 13 ...
#  $ V30: int  5 11 12 16 2 14 12 7 5 0 ...
#  $ V31: int  7 11 2 10 0 0 6 0 8 0 ...
#  $ V32: int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V33: int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V34: int  7 12 1 1 4 0 1 0 5 1 ...
#  $ V35: int  13 12 16 4 16 0 1 0 12 12 ...
#  $ V36: int  3 0 16 4 3 0 0 0 0 16 ...
#  $ V37: int  0 0 16 13 4 12 0 2 0 12 ...
#  $ V38: int  8 8 16 10 11 10 10 13 8 14 ...
#  $ V39: int  7 12 10 2 2 0 4 0 8 4 ...
#  $ V40: int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V41: int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V42: int  4 7 2 0 0 0 0 0 5 0 ...
#  $ V43: int  12 15 12 0 14 0 0 0 12 11 ...
#  $ V44: int  0 1 16 0 3 6 0 0 0 8 ...
#  $ V45: int  1 0 10 15 0 16 5 4 7 0 ...
#  $ V46: int  13 13 0 4 4 6 10 12 15 3 ...
#  $ V47: int  5 11 0 0 11 0 0 0 5 12 ...
#  $ V48: int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V49: int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V50: int  0 0 0 0 0 0 0 0 5 0 ...
#  $ V51: int  14 16 2 0 10 5 0 6 16 13 ...
#  $ V52: int  9 8 16 3 8 15 8 7 13 11 ...
#  $ V53: int  15 10 4 16 4 15 15 14 16 8 ...
#  $ V54: int  9 15 0 0 11 8 3 5 6 13 ...
#  $ V55: int  0 3 0 0 12 8 0 0 0 12 ...
#  $ V56: int  0 0 0 0 0 3 0 0 0 0 ...
#  $ V57: int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V58: int  0 0 0 0 0 0 0 0 0 0 ...
#  $ V59: int  6 10 9 0 4 10 1 4 10 3 ...
#  $ V60: int  14 16 14 1 12 16 13 13 12 15 ...
#  $ V61: int  7 15 0 15 14 16 5 8 5 11 ...
#  $ V62: int  1 3 0 2 7 16 0 0 0 6 ...
#  $ V63: int  0 0 0 0 0 16 0 0 0 0 ...
#  $ V64: int  0 0 0 0 0 6 0 0 0 0 ...
#  $ V65: int  0 0 7 4 6 2 5 5 0 8 ...


pol <- polyFit(dtrain, 2, 2, "glm", TRUE, 0.9) # about 40 min
pred <- predict(pol, dtest[,-ncol(dtest)])
mean(pred == dtest[,ncol(dtest)])
# [1] 0.8731219



nynn1 <- nnet(as.factor(V65)~., data=dtrain,size=10,decay=.001)
mean(dtest$V65 == predict(nynn1, dtest, type = "class"))
# [1] 0.8747913
nynn2 <- nnet(as.factor(V65)~., data=dtrain,size=5,decay=.001)
mean(dtest$V65 == predict(nynn2, dtest, type = "class"))
# [1] 0.7501391
nynn3 <- nnet(as.factor(V65)~., data=dtrain,size=12,decay=.001)
mean(dtest$V65 == predict(nynn3, dtest, type = "class"))
# [1] 0.8892599
nynn4 <- nnet(as.factor(V65)~., data=dtrain,size=14,decay=.001)
mean(dtest$V65 == predict(nynn4, dtest, type = "class"))
# Error in nnet.default(x, y, w, softmax = TRUE, ...) :
#   too many (1060) weights

